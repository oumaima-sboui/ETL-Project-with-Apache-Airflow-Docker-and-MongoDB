# ETL Project with Apache Airflow, Docker and MongoDB   

## Table of Contents

- [Project Description](#project-description)
- [Technologies Used](#technologies-used)
- [File Structure](#file-structure)
- [Getting Started](#getting-started)
- [Pipeline Overview](#Pipeline-Overview)
- [Results](#Results)


## Project Description

This project is an example of an ETL (Extract, Transform, Load) pipeline that is used to extract data from multiple CSV files, transform the data through merging, cleaning and preprocessing, and finally load the transformed data into a final CSV file and a MongoDB database.

The project is built using Docker and Docker Compose to ensure consistency in the environment and easy deployment. Apache Airflow is used to manage the pipeline through DAGs (Directed Acyclic Graphs) which define the workflow and dependencies between the different tasks in the pipeline.

The MongoDB database serves as the persistent storage for the transformed data, while the final CSV file can be used as a backup or for further analysis.



## Technologies Used

- Python
- Docker
- Docker Compose
- Apache Airflow
- MongoDB

## File Structure


- `dags/`: This directory contains the DAG file (`main.py`) defining the ETL pipeline.
- `logs/`: This directory contains the logs generated by Airflow, separated by component (`scheduler` and `webserver`).
- `plugins/`: This directory contains any custom Airflow plugins.
- `raw_data/`: This directory contains the raw input data in CSV format. In this project, we have three files: `clients.csv`, `hotels.csv`, and `bookings.csv`.
- `processed_data/`: This directory contains the final output data in CSV format. In this project, we have a file named `processed_data.csv`.
- `docker-compose.yml`: This file contains the configuration code for Docker Compose.
- `README.md`: This file contains information about the project.

Note that the files/directories in  `logs/` directory  and the `processed_data/` directory are not included in the repository, as they are automatically generated during runtime.


## Getting Started

1. Install Docker and Docker Compose
2. Run 'docker-compose airflow-init'  to initialize the Airflow environment in a Docker Compose setup.
3. Run `docker-compose up -d` to start the services
4. Access Airflow at http://localhost:8080
5. Trigger the DAG to start the ETL process.
7. Access the final CSV file in the `processed_data/` directory
8. Access the MongoDB database using a MongoDB client

## Pipeline Overview

The ETL pipeline consists of the following steps:
- **Extract**: The pipeline extracts data from CSV files in the `raw_data` directory.
- **Transform**: The pipeline cleans and preprocesses the extracted data by merging the client, hotel, and booking data.
- **Load to CSV**: The pipeline loads the transformed data to a final CSV file in the `processed_data` directory.
- **Load to MongoDB**: The pipeline loads the transformed data to a MongoDB database using the `pymongo` library.
- **XCom Communication**: The pipeline uses XCom to communicate between the functions in the DAG.

## Results
After running the ETL pipeline, you can verify the execution of the DAG by checking the Airflow web interface. 

1. Open a browser and navigate to http://localhost:8080. 
2. Click on the DAGs link in the top menu to view the list of available DAGs.
3. Find and click on the "booking_ingestion_dag" DAG.
4. You can view the status of each task in the DAG, as well as any logs or errors that occurred during execution.

![Airflow DAGs pipeline](https://github.com/oumaima-sboui/ETL-Project-with-Apache-Airflow-Docker-and-MongoDB/blob/master/ETL_Airflow_DAG.JPG)


After running the ETL pipeline, you should see the following results:

- **A final CSV file** named processed_data.csv in the processed_data directory, containing the cleaned and merged data from the input CSV files.

- **A MongoDB database** named "my_database" with the collections: "my_collection" containing the cleaned and merged data from the input CSV files.

To verify that the pipeline has run successfully, you can check the logs in the logs directory. If there were any errors or warnings during the pipeline execution, they will be recorded in the logs.


Overall, this project serves as an example of how to use ETL pipelines and popular technologies such as Docker, Docker Compose, Apache Airflow, and MongoDB to extract, transform, and load data from various sources.
